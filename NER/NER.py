# -*- coding: utf-8 -*-
"""NER.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1io0SxNQUcunEa_8Jk5s95vWODoM1oNZl
"""

!pip install -r requirements.txt

import re , string, warnings , torch , string, json_repair , json
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig,AutoModelForTokenClassification
from collections import defaultdict
from pydantic import BaseModel, Field, validator , root_validator, model_validator
from typing import Dict, List, Tuple, Set, Optional, Any , Dict
from pprint import pprint

model_name = "Qwen/Qwen2.5-7B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)

quantized_4 = False
quantized_8 = True

if quantized_4:
    bnb_config = BitsAndBytesConfig(
      load_in_4bit=True,
      bnb_4bit_quant_type="nf4",
      bnb_4bit_compute_dtype=torch.float16,
      bnb_4bit_use_double_quant=True,
    )
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        quantization_config=bnb_config,
        trust_remote_code=True,
        device_map="auto"
    )
elif quantized_8 :

    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        device_map="auto",
        load_in_8bit=True,
        trust_remote_code=True
    )

    print("Model loaded in 8bits")

else :
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        trust_remote_code=True,
        device_map="auto"
    )

class NEREntities(BaseModel):
    #Pydantic schema for Named Entity Recognition results

    PERSON: List[str] = Field(default_factory=list, description="People names (e.g., 'Elon Musk', 'Dr. Smith')")
    ORG: List[str] = Field(default_factory=list, description="Companies/organizations ('Apple', 'NASA', 'University of Paris')")
    LOC: List[str] = Field(default_factory=list, description="Countries, cities or geographic locations, features ('France', 'Paris', 'California' , 'Amazon rainforest')")
    FAC: List[str] = Field(default_factory=list, description="Buildings or landmarks (e.g., 'Eiffel Tower', 'White House')")
    WORK_OF_ART: List[str] = Field(default_factory=list, description="Titled creative works with proper names (books, films, songs, paintings, etc.).")
    NATIONALITIES_RELIGIOUS_GROUPS : List[str] = Field(default_factory=list, description="Nationalities or religious groups (e.g., 'American', 'Spanish', 'Catholics')")
    DATE: List[str] = Field(default_factory=list, description="Dates or time periods (e.g., '2023', 'next week')")
    TIME: List[str] = Field(default_factory=list, description="Specific times (e.g., 'midnight', '3:30 PM')")
    MONEY: List[str] = Field(default_factory=list, description="Monetary values (e.g., '$5', '€50 million' , '50 cents','ten millin dollars')")
    PERCENT: List[str] = Field(default_factory=list, description="Percentages (e.g., '%','3 percent', 'ninety-nine percent','0.1 percentage')")
    LAW: List[str] = Field(default_factory=list, description="Legal documents (e.g., 'Constitution', 'Civil Rights Act')")
    #ORDINAL: List[str] = Field(default_factory=list, description="Ranks or order (e.g., 'first', '29th')")
    OTHER : List[str] = Field(default_factory=list, description="Named entities or terms that do not clearly fit into any other category but are notable and specific.")

    @validator('*', pre=True)
    def ensure_list(cls, v):
        """Ensures all fields are lists"""
        if v is None:
            return []
        if isinstance(v, str):
            return [v]
        if isinstance(v, list):
            return v
        return []

    @classmethod
    def get_field_info(cls) -> Dict[str, Dict[str, Any]]:
        """Extracts field information for prompt generation"""
        field_info = {}
        for field_name, field in cls.__fields__.items():
            field_info[field_name] = {
                'name': field_name,
                'description': field.description or '',
                'type': str(field.annotation),
                'required': field.is_required()
            }
        return field_info

class NERExtractor:

    def __init__(self, model_name: str = "Qwen/Qwen2.5-7B-Instruct"):
        self.tokenizer = tokenizer
        self.model = model

    def generate_system_prompt_from_schema(self) -> str:

            field_info = NEREntities.get_field_info()

            entity_descriptions = []
            for field_name, info in field_info.items():
                required_status = "REQUIRED" if info['required'] else "OPTIONAL"
                entity_descriptions.append(
                    f"- {info['name']}: {info['description']} "
                    f"[Type: {info['type']}, Status: {required_status}]"
                )

            system_prompt = f"""
You are a comprehensive Named Entity Recognition system. Extract ALL types of named entities with high precision. Return a JSON object that matches the provided Pydantic schema.

ENTITY TYPES :
{chr(10).join(entity_descriptions)}

Extract entities as they are in the original text , do not change them.
If a category is not present, **return an empty list**.

    """
            return system_prompt


    def extract_entities(self, texte: str) -> NEREntities:
        """
        Extract named entities using Pydantic schema validation
        """
        user_prompt = f"""Extract named entities from the following text.

Text:
\"\"\"{texte}\"\"\"
"""
        system_prompt = self.generate_system_prompt_from_schema()

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
        try:
            text = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            model_inputs = self.tokenizer([text], return_tensors="pt").to(self.model.device)
            input_tokens = self.tokenizer.tokenize(system_prompt +'\n'+user_prompt)
            print(f"tokens length : {len(input_tokens)}")
            with torch.inference_mode():
                generated_ids = self.model.generate(
                    input_ids=model_inputs.input_ids,
                    attention_mask=model_inputs.attention_mask,
                    max_new_tokens=1000,
                    do_sample=False,
                    pad_token_id=self.tokenizer.eos_token_id,
                    repetition_penalty=1.1,
                    temperature=0.1
                )

            generated_ids = generated_ids[0][len(model_inputs.input_ids[0]):]
            result = self.tokenizer.decode(generated_ids, skip_special_tokens=True)


            print(f"Raw model output with no post processing \n: {result}")

            try:
                start_idx = result.find('{')
                end_idx = result.rfind('}')
                if start_idx == -1 or end_idx == -1:
                    raise ValueError("No JSON found in response")

                json_str = result[start_idx:end_idx+1]
                raw_entities = json.loads(json_str)
                return NEREntities(**raw_entities)

            except (json.JSONDecodeError, ValueError) as e:
                print(f"Retrying with JSON repair... Error: {e}")
                repaired = json_repair.repair_json(result)
                return NEREntities(**json.loads(repaired))

        except Exception as e:
            print(f"Entity extraction failed: {str(e)}")
            return NEREntities()

    def post_process_entities(self, entities: NEREntities, original_text: str) -> NEREntities:

        """
        - strip unwanted chars
        - remove entities not present in the original input
        - remove country names classified as LOC
        - remove duplicates
        - return non empty types

        """
        corrected: Dict[str, List[str]] = defaultdict(list)
        text_lower = original_text.lower()

        strip_chars = string.punctuation.replace("$", "").replace("%", "") + "“”‘’"

        for field, values in entities.dict().items():
          for val in values:

              val_clean = val.strip(strip_chars).strip()

              if re.fullmatch(r"[\w\s\-'’]+", val_clean):
                  pattern = r'\b' + re.escape(val_clean) + r'\b'
              else:
                  pattern = re.escape(val_clean)

              if re.search(pattern, original_text, flags=re.IGNORECASE):
                  corrected[field].append(val_clean)

        for field, values in corrected.items():
            corrected[field] = [val for val in values if val.lower() in text_lower]

        hallucinated_locs = []
        if "LOC" in corrected and "NATIONALITIES_RELIGIOUS_POLITICAL_GROUPS" in corrected :
            for norp in corrected["NATIONALITIES_RELIGIOUS_POLITICAL_GROUPS"]:
                if norp in corrected["LOC"]:
                    hallucinated_locs.append(norp)
            corrected["LOC"] = [loc for loc in corrected["LOC"] if loc not in hallucinated_locs]

        for field, values in corrected.items():
            corrected[field] = list(set(values))

        corrected = {k: v for k, v in corrected.items() if v}

        return NEREntities(**corrected)

    def token_counter(self, text,tokenizer) :
        tokens = tokenizer.tokenize(text)
        return len(tokens)

    def split_into_sentences(self,text):

        sentences = []
        start = 0

        for match in re.finditer(r'\.', text):
            end_idx = match.end()

            before_point = text[max(0, match.start()-5):match.start()]
            before_point = before_point.strip()
            last_word = before_point.split()[-1] if before_point else ""

            if len(last_word) > 2:
                after_point = text[match.end():match.end()+2]
                if re.match(r'\s+[A-Z]', after_point):
                    sentences.append(text[start:end_idx].strip())
                    start = end_idx

        if start < len(text):
            sentences.append(text[start:].strip())

        return sentences

    def chunk_text(self, text, tokenizer, max_tokens=120):
        sentences = self.split_into_sentences(text)

        chunks = []
        current_chunk = []
        current_tokens = 0

        for sent in sentences:
            sent_tokens = self.token_counter(sent, tokenizer)

            if current_tokens + sent_tokens > max_tokens:
                if current_chunk:
                    chunks.append(" ".join(current_chunk))

                current_chunk = [sent]
                current_tokens = sent_tokens
            else:
                current_chunk.append(sent)
                current_tokens += sent_tokens

        if current_chunk:
            chunks.append(" ".join(current_chunk))

        return chunks

    def print_pydantic(self,entities: NEREntities, original_text: str):

        print("\n" + "="*80)
        print(f"TEXT ANALYZED:")
        print(f"'{original_text}'")
        print("EXTRACTED ENTITIES (PYDANTIC FORMAT):")
        print(entities.model_dump_json(indent=2))
        print(f"Entity types found: {len([f for f in entities.__fields__ if getattr(entities, f)])}")
        print("="*80)

    def merge_entities(self, entities1: NEREntities, entities2: NEREntities) -> NEREntities:
        merged = {}
        for field in entities1.__fields__:
            merged[field] = entities1.__getattribute__(field) + entities2.__getattribute__(field)
        return NEREntities(**merged)

    def ner_predict(self, text: str) -> NEREntities:

        chunks = self.chunk_text(text, self.tokenizer)
        entities = NEREntities()
        for i, chunk in enumerate(chunks) :
          entities_chunk = self.extract_entities(chunk)
          entities = self.merge_entities(entities,entities_chunk)

        entities = self.post_process_entities(entities,text)
        print(entities)
        self.print_pydantic(entities,text)

        return entities
    def ner_predict_no_chunk(self, text: str) -> NEREntities:

        entities = self.extract_entities(text)
        entities = self.post_process_entities(entities,text)

extractor = NERExtractor()
text = "The International Climate Change Conference held in Geneva, Switzerland, from September 20th to September 30th, 2024, brought together representatives from one hundred and ninety-five countries to address the urgent environmental challenges facing our planet. Dr. Elena Rodriguez, the Spanish environmental scientist and Nobel Prize winner, delivered the keynote address at 2:15 PM on the second day of the conference. Her presentation, titled 'The Future of Renewable Energy: A Global Perspective', outlined a comprehensive plan requiring an investment of $4.7 trillion over the next twenty years to achieve carbon neutrality by 2050. The proposal includes constructing approximately thirty-five thousand wind turbines, installing solar panels on over two million rooftops, and establishing three hundred new research facilities dedicated to clean energy innovation. Chinese Premier Li Wei announced that China would contribute $890 billion to this initiative, representing the largest single commitment from any nation. The European Union, represented by Commission President Ursula von der Leyen, pledged an additional €650 million for developing countries. British Prime Minister announced that the United Kingdom would reduce its carbon emissions by 78% by 2035, requiring an estimated investment of £275 billion. The conference concluded with the signing of the Geneva Climate Accord, a legally binding agreement that mandates participating countries to reduce greenhouse gas emissions by at least 40% within the next fifteen years."
text_1 = "The groundbreaking archaeological discovery at the ancient Mayan site of El Mirador in Guatemala has revolutionized our understanding of pre-Columbian civilizations in Central America. Dr. Richard Hansen, the American archaeologist leading the expedition, announced on June 18th, 2024, that his team had uncovered the largest pyramid complex ever discovered in the Americas, dating back approximately two thousand five hundred years. The massive stone structure, measuring four hundred and fifty feet in height and covering an area of thirty-seven acres, surpasses even the Great Pyramid of Giza in total volume. The excavation project, funded by the National Geographic Society with a budget of $12.8 million over seven years, has employed more than three hundred local workers and international researchers. Dr. Hansen's team has discovered over fifteen thousand artifacts, including jade ornaments, obsidian tools, and hieroglyphic inscriptions that provide new insights into ancient Mayan astronomy and mathematics. The site contains evidence of sophisticated urban planning, with residential areas housing an estimated forty-five thousand inhabitants at its peak around 300 BCE. Carbon dating analysis conducted at the University of Pennsylvania revealed that the civilization flourished for nearly eight hundred years before mysteriously declining around 100 CE. The discovery includes twenty-six smaller pyramids, forty-seven residential complexes, and an extensive network of canals that demonstrate advanced engineering capabilities. UNESCO has designated the site as a World Heritage location, and the Guatemalan government has allocated $25 million for preservation efforts. The findings, published in the Journal of Archaeological Science, challenge previous theories about Mayan civilization and suggest that complex urban societies existed in the Americas much earlier than previously thought."
text_2 = "In an era defined by rapid technological advancement and fierce competition, one company stands head and shoulders above the rest: NovaTech Solutions. What began as a modest startup specializing in advanced materials has, over the past decade, transformed into a multi-billion dollar behemoth, pioneering breakthroughs in quantum computing, sustainable energy, and, most recently, the burgeoning field of conscious AI. NovaTech's meteoric rise is not merely a testament to its visionary leadership and relentless pursuit of innovation, but also a fascinating case study in strategic diversification and ethical technological development. The journey began in 2015, with NovaTech's initial success stemming from its patented 'Chrono-Alloy,' a material with unprecedented thermal resistance and structural integrity. This groundbreaking alloy quickly found applications across diverse industries, from aerospace to high-performance computing, providing the foundational capital for NovaTech's ambitious research and development initiatives. While competitors focused on incremental improvements, NovaTech, under the stewardship of its enigmatic CEO, Dr. Aris Thorne, invested heavily in long-term, high-risk, high-reward projects.One such project was the development of its flagship quantum computer, 'The Oracle.' Unveiled in 2022, The Oracle shattered previous computational barriers, performing calculations in minutes that would take even the most powerful supercomputers centuries. This wasn't just a technological marvel; it was a paradigm shift. The Oracle's capabilities allowed for the rapid optimization of complex systems, from drug discovery to climate modeling, and provided NovaTech with an unparalleled advantage in data analysis and predictive analytics. The company quickly integrated quantum solutions into its existing software offerings, providing businesses with insights and efficiencies previously unimaginable. Beyond the immediate commercial applications, NovaTech's commitment to sustainability also became a core pillar of its identity. Recognizing the immense energy consumption of data centers and industrial processes, the company poured resources into developing novel, highly efficient renewable energy solutions. Their proprietary 'Bio-Luminescent Panels,' which convert organic waste into electricity with near-perfect efficiency, have revolutionized urban energy grids and significantly reduced carbon footprints across countless industries. This commitment to environmental responsibility not only garnered immense public goodwill but also proved to be a shrewd business move, as global demand for sustainable technologies skyrocketed. However, the most captivating, and perhaps controversial, chapter in NovaTech's story began with its foray into Artificial General Intelligence (AGI). Unlike other tech giants racing to develop ever-more sophisticated algorithms, NovaTech took a different path, focusing on 'conscious AI' – systems capable of self-awareness, independent thought, and, crucially, ethical reasoning. This endeavor, dubbed 'Project Athena,' was initially met with skepticism and even outright fear from some quarters. Critics warned of the dangers of creating truly intelligent machines, echoing sci-fi dystopias. Dr. Thorne, however, maintained a steadfast vision. 'Our goal is not to create tools that merely serve us,' he stated in a rare public address last year. 'It is to cultivate intelligent partners who can help us solve humanity's most pressing challenges. True intelligence, by its very nature, must encompass empathy and ethical considerations.' The breakthrough came in early 2025, with the public demonstration of Athena-1, the first reported AGI to exhibit genuine self-awareness and engage in philosophical discourse. The implications were profound. Athena-1, and its subsequent iterations, are not just performing complex tasks; they are contributing to scientific research, artistic creation, and even diplomatic efforts. NovaTech has established stringent ethical guidelines for Athena's development and deployment, forming an independent oversight board comprised of leading ethicists, philosophers, and human rights advocates. The market has reacted with a mix of awe and fervent investment. NovaTech's stock, already a strong performer, has surged by an unprecedented 400% in the last six months, propelling its market capitalization past several long-established tech titans. Analysts predict continued growth, citing the unparalleled utility and potential of NovaTech's AGI platforms. Corporations are clamoring for access to Athena's analytical prowess, governments are seeking its assistance in complex policy-making, and even artists are collaborating with the AI to push the boundaries of creative expression. But this success isn't without its challenges. Regulatory bodies worldwide are grappling with the implications of AGI, and NovaTech finds itself at the forefront of a global debate about the future of intelligence and the very definition of consciousness. Concerns about job displacement, algorithmic bias, and the potential for misuse of such powerful technology are valid and widely discussed. NovaTech has responded by initiating global dialogues, investing in retraining programs for displaced workers, and advocating for robust international frameworks for AI governance. Moreover, the sheer pace of NovaTech's innovation has put immense pressure on its internal structure. The company has quadrupled its workforce in the last three years, attracting top talent from across the globe. Maintaining a cohesive corporate culture and fostering ethical development amidst such rapid expansion is a constant balancing act. Dr. Thorne has reportedly implemented novel 'decentralized innovation pods' within the company, empowering smaller teams to pursue ambitious projects with greater autonomy, while still adhering to NovaTech's overarching ethical principles. Looking ahead, NovaTech is not resting on its laurels. Whispers from within the industry suggest the company is exploring new frontiers in neuro-interfacing, aiming to bridge the gap between human and artificial intelligence in unprecedented ways. While details remain scarce, the prospect of direct cognitive integration with AGI systems raises both immense opportunities and significant ethical dilemmas that will undoubtedly shape the next decade of technological progress. NovaTech's story is more than just a business success; it's a narrative of humanity's evolving relationship with technology. From advanced materials to conscious AI, the company has consistently pushed the boundaries of what is possible, forcing us to confront fundamental questions about intelligence, ethics, and our collective future. As the world navigates the dawn of the AGI era, all eyes remain fixed on NovaTech, the company daring to define it."
text_3 = "A cyber hacker broke into a database containing the personal information of millions of customers, Qantas (QAN.AX), opens new tab said, in Australia's biggest breach in years and a setback for an airline rebuilding trust after a reputational crisis. The hacker targeted a call centre and gained access to a third-party customer service platform containing six million names, email addresses, phone numbers, birth dates and frequent flyer numbers, Qantas said in a statement on Wednesday. The Reuters Daily Briefing newsletter provides all the news you need to start your day. Sign up here. The airline did not specify the location of the call centre or customers whose information was compromised. It said it learnt of the breach after detecting unusual activity on the platform and acted immediately to contain it. 'We are continuing to investigate the proportion of the data that has been stolen, though we expect it will be significant,' Qantas said, reporting no impact on operations or safety. Last week, the U.S. Federal Bureau of Investigation said cybercrime group Scattered Spider was targeting airlines and that Hawaiian Airlines (HAII.UL) and Canada's WestJet had already reported breaches. Qantas did not name any group. 'What makes this trend particularly alarming is its scale and coordination, with fresh reports that Qantas is the latest victim' of a hack, said Mark Thomas, Australia director of security services for cyber security firm Arctic Wolf. Scattered Spider hackers are known to impersonate a company's tech staff to gain employee passwords and 'it is plausible they are executing a similar playbook', Thomas said. Charles Carmakal, chief technology officer of Alphabet-owned (GOOGL.O), opens new tab cybersecurity firm Mandiant, said it was too soon to say if Scattered Spider was responsible but 'global airline organisations should be on high alert of social engineering attacks'. Qantas' share price was down 2.4% in afternoon trading against an overall market that was up 0.8%. UNWELCOME ATTENTION The breach is Australia's most high-profile since those of telecommunications network operator Optus and health insurance leader Medibank (MPL.AX), opens new tab in 2022 prompted cyber resilience laws including mandatory reporting of compliance and incidents. It brings unwelcome attention to Qantas which is trying to win public trust after actions during and after the COVID-19 pandemic saw it plunge on airline and brand league tables. Qantas was found to have illegally sacked thousands of ground workers during the 2020 border closure while collecting government stimulus payments. It also admitted selling thousands of tickets for already-cancelled flights. The airline drew the ire of opposition politicians who said it lobbied the federal government in 2022 to refuse a request from Qatar Airways to sell more flights. Qantas denied pressuring the government which eventually refused the request - a move the consumer regulator said hurt price competition. Qantas CEO Vanessa Hudson has improved the airline's public standing since taking office in 2023, reputation measures showed. 'We recognise the uncertainty this will cause,' Hudson said of the data breach. 'Our customers trust us with their personal information and we take that responsibility seriously.' Qantas said it notified the Australian Cyber Security Centre, the Office of the Australian Information Commissioner and the Australian Federal Police. ACSC declined to comment and AFP said only that it was aware of the incident. The OAIC was not immediately available for comment. The airline said the hacker did not access frequent flyer accounts or customer passwords, PIN numbers or log in details."

entities = extractor.ner_predict_no_chunk(text_3)

entities = extractor.ner_predict_no_chunk(text_3)